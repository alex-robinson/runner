# runner

Sample parameters, run model ensemble over multiple processors

Requirements
============
- python 2.7 and 3
- numpy (tested with 1.11)
- scipy (tested with 0.16 and 0.18)

Install
=======
    python setup.py install

Usage
=====
Use command-line help:

- [job -h](doc/job.txt)
- [job product -h](doc/product.txt)
- [job sample -h](doc/sample.txt)
- [job resample -h](doc/resample.txt)
- [job run -h](doc/run.txt)
- [job analyze -h](doc/analyze.txt)


Examples
========

Parameter sampling
------------------

Factorial combination of parameters:

    job product a=2,3,4 b=0,1
    
     a      b
     2      0
     2      1
     3      0
     3      1
     4      0
     4      1

Monte-carlo sampling:

    job sample a=U?0,1 b=N?0,1 --size 10 --seed 4

         a      b
    0.425298236238 0.988953805595
    0.904416005793 2.62482283016
    0.68629932356 0.705445219934
    0.397627445478 -0.766770633921
    0.577938292179 -0.522609467132
    0.0967029839014 -0.14215407458
    0.71638422414 0.0495725958965
    0.26977288246 0.519632323554
    0.197268435996 -1.60068615198
    0.800898609767 -0.948326628599

The above command draws 10 samples from "a" as uniform distribution between 0 
and 1 and "b" as normal distribution of mean 0 and standard deviation 1. 
The seed parameter sets the random state, to make the sampling reproducible.
Sampling method defaults to Latin Hypercube Sampling, built on the pyDOE 
package (copied in runner to reduce external dependencies).


Run model ensemble
------------------

Given a model interface, `job run` performs ensemble simulations with full
leverage of all processors of your machine (including a cluster, if processors 
are allocated, e.g. `sbatch -n N` for SLURM, see below). The canonical form is:

    job run [-h] [-m INTERFACE] [-p [NAME=SPEC [NAME=SPEC ...]] | -i
               PARAMS_FILE] [-j I,J...,START-STOP:STEP,...]
               [-o EXPDIR] [-t TIMEOUT] [--shell] [MORE OPTIONS]

For instance, the command:

    job run -p a=2,3,4 b=0,1 -o out

runs an ensemble of 6 model versions for the parameter combinations illustrated
above for `job product`.  Parameter ensembles generated by `job sample` can
also be provided as input via `-i/--params-file` option instead of
`-p/--params`.  The `job run` parameter `-o/--out-dir` indicates experiment
directory, under which individual ensemble member run directories will be
created:

    out/                : experment directory given via `-o`
        1/              : run ID
            runner.json : runner file containing metadata + params + retrieved output
            log.out     : standard output
            log.err     : standard error
            ...         : model specific
        
        2/
    ...

There are a number of options to determine how this should be done
(e.g.  `-a/--auto-dir` to create sub-directory based on parameter names and
values).  The command is executed in the background to allow multiprocessing,
though the prompt will not be released until all runs are finished. The
standard output is saved to log files in each subdirectory. If the `--shell`
option is passed, all runs will be executed sequentially with standard output
displayed in the terminal, mostly useful for testing.
See [job run -h](doc/run.txt) for the full documentation.


Model interface
---------------

The example command assumes that a model interface `interface[.py, .json, .pickle]` 
is present in the run directory. This can also be specified via the `-m` flag.
These correspond to:

- interface.py: user-defined interface in a python module, e.g.:

    from runner.model import ModelInterface

    class MyInterface(ModelInterface):
        def setup(self, rundir, params):
            ... # e.g. write parameters to custom param files in run directory

        def postproc(self, rundir):
            ...  # read output from run directory
            return output

    model = MyInterface(COMMAND, defaults=...)  # indicate command, default params...

See also [examples/custom.py](examples/custom.py).

- interface.pickle: picked model interface, e.g. to reuse a model configuration 
of a previous `job run` command.

- interface.json: generated by `job setup`

The latter json format is intended to facilitate the generation of an interface 
for typical cases where parameters and output are passed via a common 
file format, or via command-line (for the parameters).


Examples of job setup + run
---------------------------
The canonical form of `job setup` is:

    job setup [OPTIONS] -- EXECUTABLE [OPTIONS]

where `EXECUTABLE` is your model executable or a command, followed by its
arguments. Note the `--` that separates `job setup` arguments `OPTIONS` from the
executable.  When there is no ambiguity in the command-line arguments (as seen
by python's argparse) it may be dropped. `job setup` options determine how
to communicate these parameter values to the model, and how to read the output.  

For instance with formattable command-line argument, using the `echo` command 
as executable:

    job setup -- echo --a {a} --b {b} --out {}
    job run -p a=2,3,4 b=0,1 -o out

The standard output is written in log files under `out/RUNID`:

    cat out/*/log.out

    --a 2 --b 0 --out out/0
    --a 2 --b 1 --out out/1
    --a 3 --b 0 --out out/2
    --a 3 --b 1 --out out/3
    --a 4 --b 0 --out out/4
    --a 4 --b 1 --out out/5

The command above runs an ensemble of 6 model versions, by calling 
`echo --a {a} --b {b} --out {}`  where `{a}`, `{b}` and `{}` are formatted using runtime
with parameter and directory, as displayed in the output above.

There are a number of other ways to communicate parameter values to your model
(see also `--arg-prefix` parameter, e.g. with `--arg-prefix "--{} "` to achieve
the same result with less redundancy, when parameter names match). Parameters
can also be passed via a file:

    job setup --file-name params.txt --file-type linesep --line-sep " "
    job run -p a=0,1 b=2 -o out

    cat out/*/params.txt

    a 0
    b 2
    a 1
    b 2

with a number of other file types. File types that involve grouping, such as
namelist, require a group prefix with a `.` separator in the parameter name:

    job run -p g1.a=0,1 g2.b=2. -o out --file-name params.txt --file-type namelist

    cat out/*/params.txt

    &g1
     a               = 0          
    /
    &g2
     b               = 2.0        
    /
    &g1
     a               = 1          
    /
    &g2
     b               = 2.0        
    /

Additionally, parameters can be set as environment variables via `--env-prefix`
argument (e.g. `--env-prefix ""` for direct access via `$NAME` within the
script).


Note for use on the cluster
---------------------------
The current version makes use of python's multiprocessing.Pool to handle parallel
tasks. When running on the cluster, it is up to the user to allocate ressources, via
sbatch, e.g. by simply writing the command in a bash script:

    
    # write job script
    echo job run -p a=2,3,4 b=0,1 -o out -- echo --a {a} --b {b} --out {} > jobrun.sh

    # submit with slurm with 10 procs
    sbatch -n 10 jobrun.sh
